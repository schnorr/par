* MPI + Mandelbrot Problem

All code in this section is inspired from:
- [[http://rosettacode.org/wiki/Mandelbrot_set][Mandelbrot Set @ Rosetta Code]]
- and [[https://gist.githubusercontent.com/andrejbauer/7919569/raw/258d9ec48ee2f676f0104f496c489eb9e64dca19/mandelbrot.c][this GIST]]

Let's use the Mandelbrot problem to observe the *load imbalance among
homogeneous cores* in multiple machines of the PCAD platform. The
ultimate goal is to create a parallel computer program based on MPI
where the maestro defines many mandelbrot coordinates that are
distributed equaly among players. Each player then compute the
coordinates it received from the maestro, and send back the results so
the mandelbrot image can be reconstructed. These are the steps in this
tutorial:

1. Understand how to compute one mandelbrot point
2. Understand how to compute many mandelbrot coordinates
3. Implement a MPI application
   - Maestro distributes these coordinates among players
   - Player receive coordinates, computes, send results back to the maestro
4. Run the experiment in the PCAD platform
5. Trace the behavior of the MPI application using [[https://github.com/schnorr/akypuera/][akypuera]].
6. Compute and visualize the load of each entity (maestro and players)

** Step #1: Compute one Mandelbrot point

Each (x,y) mandelbrot coordinate is computed as follows:

#+BEGIN_SRC C :tangle compute_one_mandelbrot_point.c :main no
#include <stdio.h>
int compute_mandelbrot_point (double x, double y, int maxiter)
{
  int k; /* Iteration counter */
  double u = 0.0;
  double v = 0.0;
  double u2 = u * u;
  double v2 = v * v;
  /* iterate the point */
  for (k = 1; k < maxiter && (u2 + v2 < 4.0); k++) {
    v = 2 * u * v + y;
    u = u2 - v2 + x;
    u2 = u * u;
    v2 = v * v;
  }
  return k;
}
#+END_SRC

** Step #2: Compute many mandelbrot coordinates

#+begin_src C :results output :session :exports both :tangle compute_mandelbrot_coordinates.c :main no
void compute_mandelbrot_coordinates (
  const double xmin,
  const double xmax,
  const double ymin,
  const double ymax,
  const int maxiter,
  const int xres,
  const int yres,
  double *xs, // vector of x coordinates
  double *ys  // vector of y coordinates
  )
{
  /* Precompute pixel width and height. */
  double dx=(xmax-xmin)/xres;
  double dy=(ymax-ymin)/yres;

  /* Coordinates of the current point in the complex plane. */
  double x, y; 
  /* Pixel counters */
  int i,j;

  for (j = 0; j < yres; j++) {
    y = ymax - j * dy;
    for(i = 0; i < xres; i++) {
      x = xmin + i * dx;
      ,*xs = x;
      xs++;
      ,*ys = y;
      ys++;
    }
  }
}
#+END_SRC

** Step #3: Implement the Mandelbrot MPI application
*** #3.1 (the MPI application)

The following implementation has three files:
 - ~mandelbrot-mpi.c~ (where the ~main~ function resides, see code below)
 - ~compute_mandelbrot_coordinates.c~ (function to compute mandelbrot coordinates)
 - ~compute_one_mandelbrot_point.c~ (computes the mandelbrot function)

 #+BEGIN_SRC C :tangle mandelbrot-mpi.c :main no
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>

/* function prototypes implemented in other files */
double gettime (void);
int compute_mandelbrot_point (double x, double y, int maxiter);
void compute_mandelbrot_coordinates (const double xmin,
				     const double xmax,
				     const double ymin,
				     const double ymax,
				     const int maxiter,
				     const int xres,
				     const int yres,
				     double *xs,   // vector of x coordinates
				     double *ys);  // vector of y coordinates

int main (int argc, char **argv) {
  int NP, rank;
  MPI_Init (&argc, &argv);
  MPI_Comm_size (MPI_COMM_WORLD, &NP);
  MPI_Comm_rank (MPI_COMM_WORLD, &rank);

  /* Parse the command line arguments. */
  if (argc != 7) {
    if (rank == 0){
      printf("Usage:   %s <xmin> <xmax> <ymin> <ymax> <maxiter> <xres>\n", argv[0]);
      printf("Example: %s 0.27085 0.27100 0.004640 0.004810 100 1024\n", argv[0]);
    }
    MPI_Finalize();
    return 0;
  }

  /* timing variables */
  double t0, t1;

  /* The window in the plane. */
  const double xmin = atof(argv[1]);
  const double xmax = atof(argv[2]);
  const double ymin = atof(argv[3]);
  const double ymax = atof(argv[4]);

  /* Maximum number of iterations. */
  const int maxiter = atoi(argv[5]);

  /* Image size, width is given, height is computed. */
  const int xres = atoi(argv[6]);
  const int yres = xres; //squared area

  int points = xres*yres;
  if (points % NP != 0){
    printf("Error: number of points (%d) is "
	   "not perfectly divisible by number "
	   "of players (%d)\n",
	   points, NP);
    MPI_Finalize();
    return 0;
  }

  /* Maestro print data read from parameters, for validation purposes */
  if (rank == 0){
    printf("(%f, %f) -> (%f, %f) resolution (%d, %d) maxiter %d\n",
	   xmin, ymin, xmax, ymax, xres, yres, maxiter);
  }

  /* Compute how many points per player */
  int points_per_player = -1;
  if (rank == 0){
    points_per_player = points / NP;
  }
  MPI_Bcast (&points_per_player, 1, MPI_INT, 0, MPI_COMM_WORLD);

  printf("[%d] points_per_player = %d\n", rank, points_per_player);

  printf("Broadcast OKAY\n");

  /* Malloc vectors to receive points from maestro */
  double *xs_player;
  double *ys_player;
  xs_player = malloc (points_per_player * sizeof(double));
  ys_player = malloc (points_per_player * sizeof(double));

  /* Malloc output vector for players */
  int *k_player;
  k_player = malloc (points_per_player * sizeof(int));

  double *xs = NULL, *ys = NULL, *k = NULL;
  if (rank == 0){
    /* Malloc vectors to keep ALL x and y mandelbrot coordinates */
    xs = malloc(xres*yres * sizeof(double));
    ys = malloc(xres*yres * sizeof(double));
    /* Malloc vector to get results from all players */
    k = malloc(xres*yres * sizeof(int));

    compute_mandelbrot_coordinates(xmin, xmax, ymin, ymax, maxiter, xres, yres, xs, ys);
  }

  /* scatter x coordinates among players */
  MPI_Scatter (xs, points_per_player, MPI_DOUBLE, 
	       xs_player, points_per_player, MPI_DOUBLE, 
	       0, MPI_COMM_WORLD);

  /* scatter y coordinates among players */
  MPI_Scatter (ys, points_per_player, MPI_DOUBLE,
	       ys_player, points_per_player, MPI_DOUBLE,
	       0, MPI_COMM_WORLD);

  printf("Scatter OKAY\n");

  /* Computation section (both in maestro and player) */
  t0 = gettime();
  for (int i = 0; i < points_per_player; i++){
    k_player[i] = compute_mandelbrot_point (*xs_player, *ys_player, maxiter);
    xs_player++;
    ys_player++;
  }
  t1 = gettime();

  printf("[%d] Compute OKAY %.6f secs\n", rank, t1-t0);

  MPI_Gather (k_player, points_per_player, MPI_INT,
	      k, points_per_player, MPI_INT,
	      0, MPI_COMM_WORLD);

  printf("Gather OKAY\n");
  
  MPI_Finalize ();
  return 0;
}
 #+END_SRC

*** #3.2 (timing functions)

#+BEGIN_SRC C :tangle timing_functions.c :main no
#include <sys/time.h>
#include <stddef.h>

double gettime (void)
{
  struct timeval tr;
  gettimeofday(&tr, NULL);
  return (double)tr.tv_sec+(double)tr.tv_usec/1000000;
}
#+END_SRC

*** #3.3 (the Makefile)

There is a Makefile to compile all of them in an application:

#+BEGIN_SRC makefile :tangle Makefile
CC = mpicc
CFLAGS = -O3 -g
OBJ = timing_functions.o \
      compute_mandelbrot_coordinates.o \
      compute_one_mandelbrot_point.o \
      mandelbrot-mpi.o \

%.o:%.c
	$(CC) -c -o $@ $< $(CFLAGS)

mandelbrot-mpi: $(OBJ)
	$(CC) -o $@ $^ $(CFLAGS)

clean:
	rm -f *.o
#+END_SRC

*** #3.4 The ~mandelbrot.slurm~ script

#+BEGIN_SRC bash :tangle mandelbrot.slurm
#!/bin/bash
#SBATCH --nodes=5
#SBATCH --ntasks=80
#SBATCH --time=02:00:00
#SBATCH --partition=draco
#SBATCH --workdir=.
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err

# The mandelbrot-mpi binary location
BINARY=$HOME/mandelbrot/mandelbrot-mpi

# Compile mandelbrot-mpi-akypuera
pushd $HOME/mandelbrot
rm -f $BINARY
make -f Makefile clean
make
popd

# Application parameters
XMIN=0.27085
XMAX=0.27100
YMIN=0.004640
YMAX=0.004810
MAXITER=64000
XRES=10240

# Prepare the machine file
MACHINEFILE="nodes.$SLURM_JOB_ID"
srun -l /bin/hostname | sort -n | awk '{print $2}' > $MACHINEFILE

# Get number of cores available
NP=$(cat $MACHINEFILE | wc -l)

# Execute the program
mpirun \
	--mca oob_tcp_if_include 192.168.30.0/24 \
	--mca btl_tcp_if_include 192.168.30.0/24 \
	--mca btl_base_warn_component_unused 0 \
	-np $NP \
	-machinefile $MACHINEFILE \
	$BINARY $XMIN $XMAX $YMIN $YMAX $MAXITER $XRES

echo "The SLURM script has finished (mpirun return code $?)".
#+END_SRC
*** #3.5 (Copy all these files to the PCAD platform)

Create the destination directory in PCAD

#+begin_src bash :results output :session :exports both
ssh gppd-hpc.inf.ufrgs.br mkdir -p mandelbrot
#+END_SRC

#+RESULTS:

Copy files

#+begin_src shell :results output
rsync -v \
    mandelbrot.slurm \
    compute_mandelbrot_coordinates.c \
    mandelbrot-mpi.c \
    timing_functions.c \
    compute_one_mandelbrot_point.c \
    Makefile gppd-hpc.inf.ufrgs.br:./mandelbrot
#+end_src

#+RESULTS:
: Makefile
: compute_mandelbrot_coordinates.c
: compute_one_mandelbrot_point.c
: mandelbrot-mpi.c
: mandelbrot.slurm
: timing_functions.c
: 
: sent 1,878 bytes  received 202 bytes  1,386.67 bytes/sec
: total size is 6,178  speedup is 2.97

** Step #4: Run the experiment in the PCAD platform
*** #4.1 Run the experiment

Steps to use it:
1. Connect to the frontend
   #+BEGIN_SRC bash
   ssh gppd-hpc.inf.ufrgs.br
   #+END_SRC
2. Submit the script and take note of the JobID
   #+BEGIN_SRC bash
   sbatch mandelbrot/mandelbrot.slurm
   #+END_SRC

*** #4.2 Observe the load imbalance

Verify a file ~mandelbrot.slurm_JOBID.out~.

Observe the compute time per rank.

#+begin_src shell :results output :exports both
ssh gppd-hpc.inf.ufrgs.br "cat mandelbrot.slurm_58943.out | grep Compute | head -n10"
#+end_src

#+RESULTS:
#+begin_example
[3] Compute OKAY 0.546264 secs
[4] Compute OKAY 0.579614 secs
[5] Compute OKAY 0.600088 secs
[6] Compute OKAY 0.652515 secs
[9] Compute OKAY 0.626929 secs
[8] Compute OKAY 0.650330 secs
[7] Compute OKAY 0.665047 secs
[10] Compute OKAY 0.647199 secs
[11] Compute OKAY 0.660925 secs
[12] Compute OKAY 0.654586 secs
#+end_example

Compute time (as measured by the application code itself) is about 0.6
secs for each rank. Let's see with more details:

#+name: dados_exp_mandelbrot
#+begin_src shell :results table :cache yes :exports both
ssh gppd-hpc.inf.ufrgs.br "cat mandelbrot.slurm_58976.out | grep Compute | cut -d\" \" -f1,4"
#+end_src

#+RESULTS[6530bfb8cf60375d573935ff0c2d3bf730e09e6d]: dados_exp_mandelbrot
| [3]  | 0.504004 |
| [4]  | 0.534869 |
| [5]  | 0.553545 |
| [6]  | 0.602988 |
| [8]  | 0.600034 |
| [7]  |  0.61875 |
| [10] |  0.59518 |
| [9]  | 0.604677 |
| [11] |  0.61089 |
| [12] | 0.605045 |
| [13] |  0.61146 |
| [14] | 0.621112 |
| [15] | 0.605549 |
| [16] | 0.608218 |
| [17] | 0.610561 |
| [18] | 0.683535 |
| [19] | 0.601911 |
| [20] | 0.568051 |
| [2]  | 1.263168 |
| [21] | 0.563423 |
| [22] | 0.614667 |
| [1]  |  1.52322 |
| [23] |  0.66368 |
| [24] | 0.669606 |
| [25] | 0.667163 |
| [26] | 0.705303 |
| [27] | 0.831506 |
| [30] | 0.929757 |
| [31] | 0.905309 |
| [32] | 0.898833 |
| [33] | 0.839686 |
| [34] | 0.801935 |
| [35] | 0.738701 |
| [36] | 0.657487 |
| [37] | 0.640754 |
| [38] | 0.581757 |
| [39] | 0.571014 |
| [40] | 0.642998 |
| [41] | 0.603952 |
| [42] |  0.59883 |
| [43] | 0.598321 |
| [44] | 0.572983 |
| [45] | 0.546224 |
| [46] | 0.559873 |
| [47] | 0.583018 |
| [48] | 0.598696 |
| [50] | 0.658176 |
| [51] | 0.639027 |
| [49] | 0.836836 |
| [52] | 0.682719 |
| [53] | 0.655541 |
| [54] | 0.691714 |
| [55] | 0.769031 |
| [56] | 0.681904 |
| [57] | 0.628135 |
| [58] | 0.656533 |
| [59] | 0.691172 |
| [60] | 0.751239 |
| [61] | 0.737485 |
| [62] | 0.745788 |
| [63] | 0.708546 |
| [64] | 0.689502 |
| [65] |  0.67995 |
| [29] | 3.949362 |
| [66] | 0.660264 |
| [67] | 0.682319 |
| [68] | 0.714628 |
| [69] | 0.767195 |
| [70] | 0.721229 |
| [71] | 0.676566 |
| [72] | 0.628858 |
| [73] | 0.552046 |
| [74] | 0.487409 |
| [75] | 0.440149 |
| [76] | 0.388893 |
| [77] | 0.335209 |
| [78] | 0.302255 |
| [79] | 0.244106 |
| [0]  | 0.447721 |
| [28] | 5.530829 |

Let's read this data in R and create a plot
- The X axis is the rank
- The Y axis is the compute time

But first, let's make some data handling

#+header: :var dep0=dados_exp_mandelbrot
#+begin_src R :results output :session :exports both
suppressMessages(library(tidyverse));
dep0 %>%
    as_tibble %>%
    mutate(V1 = as.integer(gsub('\\D+','', V1))) %>%
    rename(Rank = V1, Compute.Time = V2) -> df;
df;
#+end_src

#+RESULTS:
#+begin_example

# A tibble: 80 x 2
    Rank Compute.Time
   <int>        <dbl>
 1     3        0.504
 2     4        0.535
 3     5        0.554
 4     6        0.603
 5     8        0.600
 6     7        0.619
 7    10        0.595
 8     9        0.605
 9    11        0.611
10    12        0.605
# … with 70 more rows
#+end_example

Now, the plot

#+begin_src R :results output graphics :file img/mandelbrot-compute.png :exports both :width 600 :height 400 :session
df %>%
    ggplot(aes(x = Rank, y=Compute.Time)) +
    geom_point() +
    theme_bw(base_size = 22) +
    ylim(0,NA)
#+end_src

#+RESULTS:
[[file:img/mandelbrot-compute.png]]

Ow! There are some ranks with much larger compute time.

** Step #5: Trace the behavior of the MPI application
*** #5.1 Introduction

We have observed the load imbalance among ranks, but we have not yet
traced the communication time. So, let's trace the MPI application so
we can compute, per rank, the ratio between computation and
communication to inspire us to attempt to remove all the communication
footprint by masking them with computation (with asynchronous
communication). Let's employ [[https://github.com/schnorr/akypuera/][akypuera]].

*** #5.2 Installation of tracing tools (in PCAD)
**** Akypuera

Akypuera has been designed to intercept all calls to MPI and trace
when each MPI operation starts and ends. The resulting trace files
(one per process) is in the rastro binary format. You can use
=aky_converter= to generate (textual) Paje trace files.  Run the
following commands in the PCAD platform since akypuera must link
against the MPI library that is installed there. Please, make sure you
=salloc= one node because cmake is not installed in the frontend.

  #+begin_src shell :results output :exports both
git clone --recursive git://github.com/schnorr/akypuera.git
cd akypuera
mkdir build
cd build
cmake -DCMAKE_INSTALL_PREFIX=$HOME/install/akypuera ..
make install
  #+end_src

**** PajeNG

PajeNG is a framework to deal with Paje traces, such as the ones
generated by Akypuera. The =pj_dump= application enables one to
transform the (textual) file in the Paje format to a CSV-like file.
Run the following commands in the PCAD platform since ~pj_dump~ must be
available in the following steps of this tutorial. Once again, please,
make sure you =salloc= one node because cmake is not installed in the
frontend.

#+begin_src shell :results output
git clone git://github.com/schnorr/pajeng.git
mkdir -p pajeng/b
cd pajeng/b
cmake -DCMAKE_INSTALL_PREFIX=$HOME/install/pajeng/ ..
make install
#+end_src

*** #5.3 Update Makefile to link against akypuera

There is a new akypuera-enabled Makefile to compile all of them in an application:

#+BEGIN_SRC makefile :tangle Makefile.akypuera
CC = mpicc
CFLAGS = -O3 -g
LDFLAGS = -L$(HOME)/install/akypuera/lib/ -L$(HOME)/akypuera/lib/ -laky -lrastro
OBJ = timing_functions.o \
      compute_mandelbrot_coordinates.o \
      compute_one_mandelbrot_point.o \
      mandelbrot-mpi.o \

%.o:%.c
	$(CC) -c -o $@ $< $(CFLAGS)

mandelbrot-mpi-akypuera: $(OBJ)
	$(CC) -o $@ $^ $(CFLAGS) $(LDFLAGS)

clean:
	rm -f *.o
#+END_SRC

To compile, do:

#+begin_src shell :results output :exports both
make clean
make -f Makefile.akypuera
#+end_src

#+RESULTS:
: rm -f *.o
: mpicc -c -o timing_functions.o timing_functions.c -O3 -g
: mpicc -c -o compute_mandelbrot_coordinates.o compute_mandelbrot_coordinates.c -O3 -g
: mpicc -c -o compute_one_mandelbrot_point.o compute_one_mandelbrot_point.c -O3 -g
: mpicc -c -o mandelbrot-mpi.o mandelbrot-mpi.c -O3 -g
: mpicc -o mandelbrot-mpi timing_functions.o compute_mandelbrot_coordinates.o compute_one_mandelbrot_point.o mandelbrot-mpi.o -O3 -g -L/home/schnorr/install/akypuera/lib -laky -lrastro

*** #5.4 The ~mandelbrot-akypuera.slurm~ script

#+BEGIN_SRC bash :tangle mandelbrot-akypuera.slurm
#!/bin/bash
#SBATCH --nodes=5
#SBATCH --ntasks=80
#SBATCH --time=02:00:00
#SBATCH --partition=draco
#SBATCH --workdir=.
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err

# The mandelbrot-mpi binary location
BINARY=$HOME/mandelbrot/mandelbrot-mpi-akypuera

# Compile mandelbrot-mpi-akypuera
pushd $HOME/mandelbrot
rm -f $BINARY
make -f Makefile.akypuera clean
make
popd

# Application parameters
XMIN=0.27085
XMAX=0.27100
YMIN=0.004640
YMAX=0.004810
MAXITER=64000
XRES=10240

# Akypuera
export LD_LIBRARY_PATH=$HOME/install/akypuera/lib/

# Prepare the machine file
MACHINEFILE="nodes.$SLURM_JOB_ID"
srun -l /bin/hostname | sort -n | awk '{print $2}' > $MACHINEFILE

# Get number of cores available
NP=$(cat $MACHINEFILE | wc -l)

# Execute the program
mpirun \
	--mca oob_tcp_if_include 192.168.30.0/24 \
	--mca btl_tcp_if_include 192.168.30.0/24 \
	--mca btl_base_warn_component_unused 0 \
	-np $NP \
	-machinefile $MACHINEFILE \
	$BINARY $XMIN $XMAX $YMIN $YMAX $MAXITER $XRES

echo "mpirun return code is $?".

# Convert trace files to a CSV
$HOME/install/akypuera/bin/aky_converter rastro*.rst > rastro.paje
$HOME/install/pajeng/bin/pj_dump rastro.paje | grep ^State > rastro.csv

echo "The SLURM script (with akypuera) has finished."
#+END_SRC

*** #5.5 (Copy all these files to the PCAD platform)

Create the destination directory in PCAD

#+begin_src bash :results output :session :exports both
ssh gppd-hpc.inf.ufrgs.br mkdir -p mandelbrot
#+END_SRC

#+RESULTS:

Copy files

#+begin_src shell :results output
rsync -v \
    mandelbrot.slurm \
    mandelbrot-akypuera.slurm \
    compute_mandelbrot_coordinates.c \
    mandelbrot-mpi.c \
    timing_functions.c \
    compute_one_mandelbrot_point.c \
    Makefile.akypuera gppd-hpc.inf.ufrgs.br:./mandelbrot
#+end_src

#+RESULTS:
#+begin_example
Makefile.akypuera
compute_mandelbrot_coordinates.c
compute_one_mandelbrot_point.c
mandelbrot-akypuera.slurm
mandelbrot-mpi.c
mandelbrot.slurm
timing_functions.c

sent 833 bytes  received 233 bytes  710.67 bytes/sec
total size is 7,540  speedup is 7.07
#+end_example

*** #5.6 Run the experiment

Steps to use it:
1. Connect to the frontend
   #+BEGIN_SRC bash
   ssh gppd-hpc.inf.ufrgs.br
   #+END_SRC
2. Submit the script and take note of the JobID
   #+BEGIN_SRC bash
   sbatch mandelbrot/mandelbrot-akypuera.slurm   
   #+END_SRC

Copy the ~rastro.csv~ to your laptop.

** Step #6: Compute and visualize the load of maestro/players
*** #6.1 Compute time                                                :ATTACH:
    :PROPERTIES:
    :Attachments: rastro.csv
    :ID:       4791e59c-84d1-4906-bb61-7b6eba1f8531
    :END:

Read data

#+begin_src R :results output :session :exports both
library(tidyverse)
df <- read_csv("data/47/91e59c-84d1-4906-bb61-7b6eba1f8531/rastro.csv", col_names = FALSE) %>%
    mutate(Rank = gsub("rank", "", X2)) %>%
    select(-X1, -X2, -X3, -X7) %>%
    rename(Start = X4,
           End = X5,
           Duration = X6,
           Value = X8) %>%
    mutate(Rank = as.integer(Rank)) %>%
    select(Rank, everything())
df
#+end_src

#+RESULTS:
#+begin_example

Parsed with column specification:
cols(
  X1 = col_character(),
  X2 = col_character(),
  X3 = col_character(),
  X4 = col_double(),
  X5 = col_double(),
  X6 = col_double(),
  X7 = col_double(),
  X8 = col_character()
)

# A tibble: 560 x 5
    Rank    Start      End Duration Value        
   <int>    <dbl>    <dbl>    <dbl> <chr>        
 1     9  0.00182  0.00182 0.000001 MPI_Comm_size
 2     9  0.00182  0.00182 0        MPI_Comm_rank
 3     9  0.00182  0.0140  0.0122   MPI_Bcast    
 4     9  0.0140   1.13    1.12     MPI_Scatter  
 5     9  1.13     7.95    6.81     MPI_Scatter  
 6     9  8.57    15.0     6.46     MPI_Gather   
 7     9 15.0     18.4     3.40     MPI_Finalize 
 8     8  0.00322  0.00322 0.000001 MPI_Comm_size
 9     8  0.00322  0.00322 0        MPI_Comm_rank
10     8  0.00323  0.00694 0.00371  MPI_Bcast    
# … with 550 more rows
#+end_example

Compute MPI Time

#+begin_src R :results output :session :exports both
df %>%
    filter(Value != "MPI_Finalize") %>%
    group_by(Rank) %>%
    summarize(MPI.Time = sum(Duration),
              Full.Time = max(End) - min(Start)) %>%
    mutate(Compute.Time = Full.Time - MPI.Time) %>%
    mutate(Comm.Ratio = MPI.Time / Full.Time * 100) %>%
    arrange(Rank) -> df.timings;
df.timings
#+end_src

#+RESULTS:
#+begin_example

# A tibble: 80 x 5
    Rank MPI.Time Full.Time Compute.Time Comm.Ratio
   <int>    <dbl>     <dbl>        <dbl>      <dbl>
 1     0     16.9      18.4        1.53        91.7
 2     1     13.3      15.0        1.75        88.4
 3     2     13.6      15.0        1.43        90.4
 4     3     14.5      15.0        0.548       96.4
 5     4     14.4      15.0        0.581       96.1
 6     5     14.4      15.0        0.600       96.0
 7     6     14.4      15.0        0.652       95.7
 8     7     14.4      15.0        0.665       95.6
 9     8     14.4      15.0        0.649       95.7
10     9     14.4      15.0        0.627       95.8
# … with 70 more rows
#+end_example

Clearly there is too much communication in this run.

Let's visualize only the compute time.

#+begin_src R :results output graphics :file img/mandelbrot-compute-aky.png :exports both :width 600 :height 400 :session
df.timings %>%
    ggplot(aes(x = Rank, y=Compute.Time)) +
    geom_point() +
    theme_bw(base_size = 22) +
    ylim(0,NA)
#+end_src

#+RESULTS:
[[file:img/mandelbrot-compute-aky.png]]


Again, we once again have a significant amount of load imbalance.

*** #6.2 Communication time

Run the previous code block (see #6.1) to read data.

Let's visualize the MPI time, which is basically communication time.

#+begin_src R :results output graphics :file img/mandelbrot-communication-aky.png :exports both :width 600 :height 400 :session
df.timings %>%
    ggplot(aes(x = Rank, y=MPI.Time)) +
    geom_point() +
    theme_bw(base_size = 22) +
    ylim(0,NA)
#+end_src

#+RESULTS:
[[file:img/mandelbrot-communication-aky.png]]


We confirme once again the huge communication time.

There are some disparities among ranks as well.

*** #6.3 Per-MPI operation time

Run the previous code block (see #6.1) to read data.

#+begin_src R :results output graphics :file img/mandelbrot-per-communication-aky.png :exports both :width 800 :height 400 :session
df %>%
    group_by(Rank, Value) %>%
    summarize(Time = sum(Duration)) %>%
    ggplot(aes(x = Rank, y = Time, fill=Value)) +
    geom_bar(stat='identity', width=1) +
    scale_fill_brewer(palette="Set1") +
    theme_bw(base_size = 22) +
    ylim(0,NA)
#+end_src

#+RESULTS:
[[file:img/mandelbrot-per-communication-aky.png]]

We can see that the ~MPI_Scatter~ (yellow) operation takes most of the
time.  We can also see that the "anomaly" in the middle appears in
those processes that computed much more than the others. But this plot
shows only communication-related operations. Let's combine this
information with compute time, but for that we need some R wizardry:

1. Calculate compute time per-rank and _save_.
#+begin_src R :results output :session :exports both
df %>%
    group_by(Rank) %>%
    summarize(Time = max(End) - min(Start) - sum(Duration)) %>%
    mutate(Value = "Compute") -> df.compute
df.compute
#+end_src

#+RESULTS:
#+begin_example

# A tibble: 80 x 3
    Rank  Time Value  
   <int> <dbl> <chr>  
 1     0 1.53  Compute
 2     1 1.75  Compute
 3     2 1.43  Compute
 4     3 0.548 Compute
 5     4 0.581 Compute
 6     5 0.600 Compute
 7     6 0.652 Compute
 8     7 0.665 Compute
 9     8 0.649 Compute
10     9 0.627 Compute
# … with 70 more rows
#+end_example

2. Calculate per-communication operation time and per-rank and _save_.
#+begin_src R :results output :session :exports both
df %>%
    group_by(Rank, Value) %>%
    summarize(Time = sum(Duration)) -> df.communication;
df.communication
#+end_src

#+RESULTS:
#+begin_example

# A tibble: 480 x 3
# Groups:   Rank [80]
    Rank Value              Time
   <int> <chr>             <dbl>
 1     0 MPI_Bcast      0.0112  
 2     0 MPI_Comm_rank  0       
 3     0 MPI_Comm_size  0.000001
 4     0 MPI_Finalize   0.00562 
 5     0 MPI_Gather     3.43    
 6     0 MPI_Scatter   13.5     
 7     1 MPI_Bcast      0.0129  
 8     1 MPI_Comm_rank  0       
 9     1 MPI_Comm_size  0.000001
10     1 MPI_Finalize   3.43    
# … with 470 more rows
#+end_example

3. Bind rows and plot everything

#+begin_src R :results output graphics :file img/mandelbrot-per-ops-aky.png :exports both :width 800 :height 400 :session
df.compute %>%
    bind_rows(df.communication) %>%
    ggplot(aes(x = Rank, y = Time, fill=Value)) +
    geom_bar(stat='identity', width=1) +
    scale_fill_brewer(palette="Set1") +
    theme_bw(base_size = 22) +
    ylim(0,NA)
#+end_src

#+RESULTS:
[[file:img/mandelbrot-per-ops-aky.png]]

Well, colors have changed from previous plot (~MPI_Scatter~ is now
brown), but we can see the compute time (as the red color).

** Step #7: Do something yourself

Some suggestions to go forward.

*** Simple

1. What should be the parameters to make the compute time larger than
   the communication time? Is that even possible?
2. Change the problem size
   - By increasing the resolution (xres)
   - By increasing the selected mandelbrot square (xmin, ymin, xmax, ymax)
   - By increasing the max iteration (maxiter)
3. Change the number of cores
   - Using more machines
   - Attempt an heterogeneous allocation (multi-partition in PCAD)

Then analyze everything once again.

*** Intermediary

1. Design an experiment Weak scaling analysis

*** Advanced

1. For a scenario where compute time is larger the communication time,
   mask all communications by using asynchronous operations. Is this
   possible with collective operations?
2. Make all ranks compute their own coordinates to avoid communication
   of points. So by doing this, the application will end up with a
   single ~MPI_Gather~ and the end to collect results in a single rank
   (the maestro).
