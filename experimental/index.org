* Experimental codes
** MPI + Slurm in PCAD

PCAD is here:
http://gppd-hpc.inf.ufrgs.br/

Slurm script as registered in [[./hello.slurm]].

#+BEGIN_SRC bash :tangle hello.slurm
#!/bin/bash
#SBATCH --nodes=5
#SBATCH --ntasks=80
#SBATCH --time=02:00:00
#SBATCH --partition=draco
#SBATCH --workdir=.
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err

# Prepare the machien file
MACHINEFILE="nodes.$SLURM_JOB_ID"
srun -l /bin/hostname | sort -n | awk '{print $2}' > $MACHINEFILE

# Get number of cores available
NP=$(cat $MACHINEFILE | wc -l)

# The source code of our hello world program
echo "#include<stdio.h>
#include<mpi.h>
#include<unistd.h>
int main(int argc, char* argv[]) {
  int NP, rank;
  MPI_Init (&argc, &argv);
  MPI_Comm_size (MPI_COMM_WORLD, &NP);
  MPI_Comm_rank (MPI_COMM_WORLD, &rank);
  printf (\"Oi pessoal, do processo %d dentro de %d!\n\", rank, NP);
  MPI_Finalize ();
  return 0;
}" > hello.c

# Compile the program
mpicc hello.c -o hello

# Execute the program
mpirun \
	--mca oob_tcp_if_include 192.168.30.0/24 \
	--mca btl_tcp_if_include 192.168.30.0/24 \
	--mca btl_base_warn_component_unused 0 \
	-np $NP \
	-machinefile $MACHINEFILE \
	./hello
#+END_SRC

Steps to use it:
1. Copy this file to the gppd-hpc frontend
   #+BEGIN_SRC bash
   scp hello.slurm gppd-hpc.inf.ufrgs.br:.
   #+END_SRC
2. Connect to the frontend
   #+BEGIN_SRC bash
   ssh gppd-hpc.inf.ufrgs.br
   #+END_SRC
3. Submit the script and take note of the JobID
   #+BEGIN_SRC bash
   sbatch hello.slurm   
   #+END_SRC
4. Assuming your JobId is 58876, verify the contents of the file
   #+BEGIN_SRC bash
   cat nodes.58876
   #+END_SRC
5. Then, check the stdout log
   #+BEGIN_SRC bash
   cat draco.slurm_58876.out
   #+END_SRC
6. Make sure you have something that looks like this
   #+BEGIN_EXAMPLE
   ...
   Oi pessoal, do processo 19 dentro de 80!
   Oi pessoal, do processo 76 dentro de 80!
   ...
   #+END_EXAMPLE
** MPI + Mandelbrot Problem

All code in this section is inspired from:
- http://rosettacode.org/wiki/Mandelbrot_set
- and [[https://gist.githubusercontent.com/andrejbauer/7919569/raw/258d9ec48ee2f676f0104f496c489eb9e64dca19/mandelbrot.c][this GIST]]

Let's use the Mandelbrot problem to observe the *load imbalance among
homogeneous cores* in multiple machines of the PCAD platform. The
ultimate goal is to create a parallel computer program based on MPI
where the maestro defines many mandelbrot coordinates that are
distributed equaly among players. Each player then compute the
coordinates it received from the maestro, and send back the results so
the mandelbrot image can be reconstructed. These are the steps in this
tutorial:

1. Understand how to compute one mandelbrot point
2. Understand how to compute many mandelbrot coordinates
3. Implement a MPI application
   - Maestro distributes these coordinates among players
   - Player receive coordinates, computes, send results back to the maestro
4. Run the experiment in the PCAD platform
5. Trace the behavior of the MPI application using [[https://github.com/schnorr/akypuera/][akypuera]].
6. Compute and visualize the load of each entity (maestro and players)

*** Step #1: Compute one Mandelbrot point

Each (x,y) mandelbrot coordinate is computed as follows:

#+BEGIN_SRC C :tangle compute_one_mandelbrot_point.c :main no
#include <stdio.h>
int compute_mandelbrot_point (double x, double y, int maxiter)
{
  int k; /* Iteration counter */
  double u = 0.0;
  double v = 0.0;
  double u2 = u * u;
  double v2 = v * v;
  /* iterate the point */
  for (k = 1; k < maxiter && (u2 + v2 < 4.0); k++) {
    v = 2 * u * v + y;
    u = u2 - v2 + x;
    u2 = u * u;
    v2 = v * v;
  }
  return k;
}
#+END_SRC

*** Step #2: Compute many mandelbrot coordinates

#+begin_src C :results output :session :exports both :tangle compute_mandelbrot_coordinates.c :main no
void compute_mandelbrot_coordinates (
  const double xmin,
  const double xmax,
  const double ymin,
  const double ymax,
  const int maxiter,
  const int xres,
  const int yres,
  double *xs, // vector of x coordinates
  double *ys  // vector of y coordinates
  )
{
  /* Precompute pixel width and height. */
  double dx=(xmax-xmin)/xres;
  double dy=(ymax-ymin)/yres;

  /* Coordinates of the current point in the complex plane. */
  double x, y; 
  /* Pixel counters */
  int i,j;

  for (j = 0; j < yres; j++) {
    y = ymax - j * dy;
    for(i = 0; i < xres; i++) {
      x = xmin + i * dx;
      ,*xs = x;
      xs++;
      ,*ys = y;
      ys++;
    }
  }
}
#+END_SRC

*** Step #3: Implement a MPI application
**** #3.1 (the MPI application)

The following implementation has three files:
 - ~mandelbrot-mpi.c~ (where the ~main~ function resides, see code below)
 - ~compute_mandelbrot_coordinates.c~ (function to compute mandelbrot coordinates)
 - ~compute_one_mandelbrot_point.c~ (computes the mandelbrot function)

 #+BEGIN_SRC C :tangle mandelbrot-mpi.c :main no
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>

/* function prototypes implemented in other files */
double gettime (void);
int compute_mandelbrot_point (double x, double y, int maxiter);
void compute_mandelbrot_coordinates (const double xmin,
				     const double xmax,
				     const double ymin,
				     const double ymax,
				     const int maxiter,
				     const int xres,
				     const int yres,
				     double *xs,   // vector of x coordinates
				     double *ys);  // vector of y coordinates

int main (int argc, char **argv) {
  int NP, rank;
  MPI_Init (&argc, &argv);
  MPI_Comm_size (MPI_COMM_WORLD, &NP);
  MPI_Comm_rank (MPI_COMM_WORLD, &rank);

  /* Parse the command line arguments. */
  if (argc != 7) {
    if (rank == 0){
      printf("Usage:   %s <xmin> <xmax> <ymin> <ymax> <maxiter> <xres>\n", argv[0]);
      printf("Example: %s 0.27085 0.27100 0.004640 0.004810 100 1024\n", argv[0]);
    }
    MPI_Finalize();
    return 0;
  }

  /* timing variables */
  double t0, t1;

  /* The window in the plane. */
  const double xmin = atof(argv[1]);
  const double xmax = atof(argv[2]);
  const double ymin = atof(argv[3]);
  const double ymax = atof(argv[4]);

  /* Maximum number of iterations. */
  const int maxiter = atoi(argv[5]);

  /* Image size, width is given, height is computed. */
  const int xres = atoi(argv[6]);
  const int yres = (xres*(ymax-ymin))/(xmax-xmin);

  int points = xres*yres;
  if (points % NP != 0){
    printf("Error: number of points (%d) is "
	   "not perfectly divisible by number "
	   "of players (%d)\n",
	   points, NP);
    MPI_Finalize();
  }

  /* Maestro print data read from parameters, for validation purposes */
  if (rank == 0){
    printf("(%f, %f) -> (%f, %f) resolution (%d, %d) maxiter %d\n",
	   xmin, ymin, xmax, ymax, xres, yres, maxiter);
  }

  /* Compute how many points per player */
  int points_per_player = -1;
  if (rank == 0){
    points_per_player = points / NP;
  }
  MPI_Bcast (&points_per_player, 1, MPI_INT, 0, MPI_COMM_WORLD);

  printf("[%d] points_per_player = %d\n", rank, points_per_player);

  printf("Broadcast OKAY\n");

  /* Malloc vectors to receive points from maestro */
  double *xs_player;
  double *ys_player;
  xs_player = malloc (points_per_player * sizeof(double));
  ys_player = malloc (points_per_player * sizeof(double));

  /* Malloc output vector for players */
  int *k_player;
  k_player = malloc (points_per_player * sizeof(int));

  double *xs = NULL, *ys = NULL, *k = NULL;
  if (rank == 0){
    /* Malloc vectors to keep ALL x and y mandelbrot coordinates */
    xs = malloc(xres*yres * sizeof(double));
    ys = malloc(xres*yres * sizeof(double));
    /* Malloc vector to get results from all players */
    k = malloc(xres*yres * sizeof(int));

    compute_mandelbrot_coordinates(xmin, xmax, ymin, ymax, maxiter, xres, yres, xs, ys);
  }

  /* scatter x coordinates among players */
  MPI_Scatter (xs, points_per_player, MPI_DOUBLE, 
	       xs_player, points_per_player, MPI_DOUBLE, 
	       0, MPI_COMM_WORLD);

  /* scatter y coordinates among players */
  MPI_Scatter (ys, points_per_player, MPI_DOUBLE,
	       ys_player, points_per_player, MPI_DOUBLE,
	       0, MPI_COMM_WORLD);

  printf("Scatter OKAY\n");

  /* Computation section (both in maestro and player) */
  t0 = gettime();
  for (int i = 0; i < points_per_player; i++){
    int k = compute_mandelbrot_point (*xs_player, *ys_player, maxiter);
    //register k in the output vector
    xs_player++;
    ys_player++;
  }
  t1 = gettime();

  printf("[%d] Compute OKAY %.6f secs\n", rank, t1-t0);

  MPI_Gather (k_player, points_per_player, MPI_INT,
	      k, points_per_player, MPI_INT,
	      0, MPI_COMM_WORLD);

  printf("Gather OKAY\n");
  
  MPI_Finalize ();
  return 0;
}
 #+END_SRC

**** #3.2 (timing functions)

 #+BEGIN_SRC C :tangle timing_functions.c :main no
#include <sys/time.h>
#include <stddef.h>

double gettime (void)
{
  struct timeval tr;
  gettimeofday(&tr, NULL);
  return (double)tr.tv_sec+(double)tr.tv_usec/1000000;
}
 #+END_SRC

**** #3.3 (the Makefile)

There is a Makefile to compile all of them in an application:

#+BEGIN_SRC makefile :tangle Makefile
CC = mpicc
CFLAGS = -O3 -g
OBJ = timing_functions.o \
      compute_mandelbrot_coordinates.o \
      compute_one_mandelbrot_point.o \
      mandelbrot-mpi.o \

%.o:%.c
	$(CC) -c -o $@ $< $(CFLAGS)

mandelbrot-mpi: $(OBJ)
	$(CC) -o $@ $^ $(CFLAGS)

clean:
	rm -f *.o
#+END_SRC

**** #3.4 (Copy all these files to the PCAD platform)

#+BEGIN_SRC bash
ssh gppd-hpc.inf.ufrgs.br mkdir -p mandelbrot
scp compute_mandelbrot_coordinates.c \
    mandelbrot-mpi.c \
    timing_functions.c \
    compute_one_mandelbrot_point.c \
    Makefile gppd-hpc.inf.ufrgs.br:./mandelbrot
ssh gppd-hpc.inf.ufrgs.br make -C mandelbrot/
#+END_SRC

*** Step #4: Run the experiment in the PCAD platform
**** #4.1 The ~mandelbrot.slurm~ script

#+BEGIN_SRC bash :tangle mandelbrot.slurm
#!/bin/bash
#SBATCH --nodes=5
#SBATCH --ntasks=80
#SBATCH --time=02:00:00
#SBATCH --partition=draco
#SBATCH --workdir=.
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err

# The mandelbrot-mpi binary location
BINARY=$HOME/mandelbrot/mandelbrot-mpi

# Application parameters
XMIN=0.27085
XMAX=0.27100
YMIN=0.004640
YMAX=0.004810
MAXITER=64000
XRES=10240

# Prepare the machine file
MACHINEFILE="nodes.$SLURM_JOB_ID"
srun -l /bin/hostname | sort -n | awk '{print $2}' > $MACHINEFILE

# Get number of cores available
NP=$(cat $MACHINEFILE | wc -l)

# Execute the program
mpirun \
	--mca oob_tcp_if_include 192.168.30.0/24 \
	--mca btl_tcp_if_include 192.168.30.0/24 \
	--mca btl_base_warn_component_unused 0 \
	-np $NP \
	-machinefile $MACHINEFILE \
	$BINARY $XMIN $XMAX $YMIN $YMAX $MAXITER $XRES
#+END_SRC
**** #4.2 Run the experiment

Steps to use it:
1. Copy this file to the gppd-hpc frontend
   #+BEGIN_SRC bash
   ssh gppd-hpc.inf.ufrgs.br mkdir -p mandelbrot
   scp mandelbrot.slurm gppd-hpc.inf.ufrgs.br:./mandelbrot/
   #+END_SRC
2. Connect to the frontend
   #+BEGIN_SRC bash
   ssh gppd-hpc.inf.ufrgs.br
   #+END_SRC
3. Submit the script and take note of the JobID
   #+BEGIN_SRC bash
   sbatch mandelbrot/mandelbrot.slurm   
   #+END_SRC
4. Assuming your JobId is 58876, verify the contents of the file
   #+BEGIN_SRC bash
   cat nodes.58876
   #+END_SRC
5. Then, check the stdout log
   #+BEGIN_SRC bash
   cat draco.slurm_58876.out
   #+END_SRC
6. Make sure you have something that looks like this
   #+BEGIN_EXAMPLE

   #+END_EXAMPLE
**** #4.3 Observe the load imbalance

Verify a file ~mandelbrot.slurm_JOBID.out~.

Observe the compute time per rank.

#+begin_src shell :results both :export both
ssh gppd-hpc.inf.ufrgs.br "cat mandelbrot.slurm_58943.out | grep Compute | head -n10"
#+end_src

#+RESULTS:
#+begin_example
[3] Compute OKAY 0.546264 secs
[4] Compute OKAY 0.579614 secs
[5] Compute OKAY 0.600088 secs
[6] Compute OKAY 0.652515 secs
[9] Compute OKAY 0.626929 secs
[8] Compute OKAY 0.650330 secs
[7] Compute OKAY 0.665047 secs
[10] Compute OKAY 0.647199 secs
[11] Compute OKAY 0.660925 secs
[12] Compute OKAY 0.654586 secs
#+end_example

*** Step #5: Trace the behavior of the MPI application
**** #5.1 Introduction

 We have observed the load imbalance among ranks, but we have not yet
 traced the communication time. So, let's trace the MPI application so
 we can compute, per rank, the ratio between computation and
 communication to inspire us to attempt to remove all the communication
 footprint by masking them with computation (with asynchronous
 communication). Let's employ [[https://github.com/schnorr/akypuera/][akypuera]].

**** #5.2 Akypuera Installation (PCAD)

Run the following commands in the PCAD platform since akypuera must
link against the MPI library that is installed there. Make sure you
=salloc= one node because cmake is not installed in the frontend.

 #+begin_src shell :results output :export both
git clone --recursive git://github.com/schnorr/akypuera.git
cd akypuera
mkdir build
cd build
cmake -DCMAKE_INSTALL_PREFIX=$HOME/install/akypuera ..
make install
 #+end_src

**** #5.3 PajeNG Installation (PCAD)

Run the following commands in the PCAD platform since ~pj_dump~ must be
available in the following steps. Make sure you =salloc= one node
because cmake is not installed in the frontend.

#+begin_src shell :results output
git clone git://github.com/schnorr/pajeng.git
mkdir -p pajeng/b
cd pajeng/b
cmake -DCMAKE_INSTALL_PREFIX=$HOME/install/pajeng/ ..
make install
#+end_src

**** #5.4 Update Makefile to link against akypuera

There is a new akypuera-enabled Makefile to compile all of them in an application:

#+BEGIN_SRC makefile :tangle Makefile.akypuera
CC = mpicc
CFLAGS = -O3 -g
LDFLAGS = -L$(HOME)/install/akypuera/lib/ -L$(HOME)/akypuera/lib/ -laky -lrastro
OBJ = timing_functions.o \
      compute_mandelbrot_coordinates.o \
      compute_one_mandelbrot_point.o \
      mandelbrot-mpi.o \

%.o:%.c
	$(CC) -c -o $@ $< $(CFLAGS)

mandelbrot-mpi: $(OBJ)
	$(CC) -o $@ $^ $(CFLAGS) $(LDFLAGS)

clean:
	rm -f *.o
#+END_SRC

To compile, do:

#+begin_src shell :results output :export
make clean
make -f Makefile.akypuera
#+end_src

#+RESULTS:
: rm -f *.o
: mpicc -c -o timing_functions.o timing_functions.c -O3 -g
: mpicc -c -o compute_mandelbrot_coordinates.o compute_mandelbrot_coordinates.c -O3 -g
: mpicc -c -o compute_one_mandelbrot_point.o compute_one_mandelbrot_point.c -O3 -g
: mpicc -c -o mandelbrot-mpi.o mandelbrot-mpi.c -O3 -g
: mpicc -o mandelbrot-mpi timing_functions.o compute_mandelbrot_coordinates.o compute_one_mandelbrot_point.o mandelbrot-mpi.o -O3 -g -L/home/schnorr/install/akypuera/lib -laky -lrastro

**** #5.5 (Copy all these files to the PCAD platform)

#+BEGIN_SRC bash
ssh gppd-hpc.inf.ufrgs.br mkdir -p mandelbrot
scp -r \
    compute_mandelbrot_coordinates.c \
    mandelbrot-mpi.c \
    timing_functions.c \
    compute_one_mandelbrot_point.c \
    Makefile.akypuera gppd-hpc.inf.ufrgs.br:./mandelbrot
rsync --recursive $HOME/install/akypuera gppd-hpc.inf.ufrgs.br:./
ssh gppd-hpc.inf.ufrgs.br make -C mandelbrot/ -f Makefile.akypuera
#+END_SRC

**** #5.6 The ~mandelbrot-akypuera.slurm~ script

#+BEGIN_SRC bash :tangle mandelbrot-akypuera.slurm
#!/bin/bash
#SBATCH --nodes=5
#SBATCH --ntasks=80
#SBATCH --time=02:00:00
#SBATCH --partition=draco
#SBATCH --workdir=.
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err

# The mandelbrot-mpi binary location
BINARY=$HOME/mandelbrot/mandelbrot-mpi

# Application parameters
XMIN=0.27085
XMAX=0.27100
YMIN=0.004640
YMAX=0.004810
MAXITER=64000
XRES=10240

# Akypuera
export LD_LIBRARY_PATH=$HOME/install/akypuera/lib/

# Prepare the machine file
MACHINEFILE="nodes.$SLURM_JOB_ID"
srun -l /bin/hostname | sort -n | awk '{print $2}' > $MACHINEFILE

# Get number of cores available
NP=$(cat $MACHINEFILE | wc -l)

# Execute the program
mpirun \
	--mca oob_tcp_if_include 192.168.30.0/24 \
	--mca btl_tcp_if_include 192.168.30.0/24 \
	--mca btl_base_warn_component_unused 0 \
	-np $NP \
	-machinefile $MACHINEFILE \
	$BINARY $XMIN $XMAX $YMIN $YMAX $MAXITER $XRES
#+END_SRC

**** #5.7 Run the experiment

Steps to use it:
1. Copy this file to the gppd-hpc frontend
   #+BEGIN_SRC bash
   ssh gppd-hpc.inf.ufrgs.br mkdir -p mandelbrot
   scp mandelbrot-akypuera.slurm gppd-hpc.inf.ufrgs.br:./mandelbrot/
   #+END_SRC
2. Connect to the frontend
   #+BEGIN_SRC bash
   ssh gppd-hpc.inf.ufrgs.br
   #+END_SRC
3. Submit the script and take note of the JobID
   #+BEGIN_SRC bash
   sbatch mandelbrot/mandelbrot-akypuera.slurm   
   #+END_SRC
4. Assuming your JobId is 58876, verify the contents of the file
   #+BEGIN_SRC bash
   cat nodes.58876
   #+END_SRC
5. Then, check the stdout log
   #+BEGIN_SRC bash
   cat draco.slurm_58876.out
   #+END_SRC
6. Make sure you have something that looks like this

**** #5.8 Convert trace files to a CSV

Run the following commands in a ~salloc~ allocation.

#+begin_src shell :results output
$HOME/install/akypuera/bin/aky_converter rastro*.rst > rastro.paje
$HOME/install/pajeng/bin/pj_dump rastro.paje | grep ^State > rastro.csv
#+end_src

Copy the ~rastro.csv~ to your laptop.

*** Step #6: Compute and visualize the load of maestro/players
